{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import packages and modules\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from keras.layers.core import Flatten, Dense, Dropout,Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from scipy.misc import imread, imresize\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn import cross_validation\n",
    "\n",
    "def count_num_files(path):\n",
    "    return(sum([len(files) for files in os.walk(path)]))\n",
    "\n",
    "def list_fileNames(path):\n",
    "    filenames=[]\n",
    "    for path, subdirs, files in os.walk(path):\n",
    "        filenames.extend([filename for filename in files])\n",
    "    return(filenames)\n",
    "\n",
    "def split_data(X,y,val_proportion):\n",
    "    X_train,X_Val,y_train,y_Val=train_test_split(X, y,test_size=val_proportion, random_state=0)\n",
    "    return X_train, X_Val, y_train, y_Val;\n",
    "\n",
    "# path to the model weights file.\n",
    "weights_path = 'vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224,224\n",
    "nb_epoch = 25\n",
    "\n",
    "# directory of our data.\n",
    "train_data_dir = 'data/train'\n",
    "test_data_dir='data/test'\n",
    "\n",
    "train_filenames = np.array(list_fileNames(train_data_dir))\n",
    "labels= train_filenames.view(np.chararray).ljust(3)\n",
    "train_labels=np.array(list(map(lambda x:(0 if x=='cat' else 1),labels)))\n",
    "\n",
    "test_filenames = np.array(list_fileNames(test_data_dir))\n",
    "labels= test_filenames.view(np.chararray).ljust(3)\n",
    "test_labels=np.array(list(map(lambda x:(0 if x=='cat' else 1),labels)))\n",
    "\n",
    "nb_train_samples =train_labels.shape[0]\n",
    "nb_test_samples=test_labels.shape[0]\n",
    "\n",
    "#print(nb_train_samples,nb_test_samples)\n",
    "#print(train_labels.sum(),test_labels.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_VGG16():\n",
    "    # build the VGG16 network\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # load the weights of the VGG16 networks\n",
    "    # (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "    # note: when there is a complete match between your model definition\n",
    "    # and your weight savefile, you can simply call model.load_weights(filename)\n",
    "    assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "    f = h5py.File(weights_path)\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "    return (model)\n",
    "\n",
    "\n",
    "def feature_extractor(model,path,nb_samples):\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    generator = datagen.flow_from_directory(\n",
    "            path,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "    bottleneck_features = model.predict_generator(generator, nb_samples)\n",
    "    return (bottleneck_features)\n",
    "    \n",
    "def save_bottlebeck_features():\n",
    "    model=load_VGG16()\n",
    "    \n",
    "    bottleneck_features_train = feature_extractor(model,train_data_dir,nb_train_samples)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    " \n",
    "    bottleneck_features_test = feature_extractor(model,test_data_dir,nb_test_samples)\n",
    "    np.save(open('bottleneck_features_test.npy', 'wb'), bottleneck_features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 0s - loss: 7.3125 - acc: 0.4987 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 0s - loss: 8.0188 - acc: 0.5025 - val_loss: 8.2202 - val_acc: 0.4900\n"
     ]
    }
   ],
   "source": [
    "#Training model for linear claasifier\n",
    "def load_linear_model(input_data_shape=1,train=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_linear_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    X_train, X_Val, y_train, y_Val=split_data(train_data,train_labels,0.2)\n",
    "    top_model = load_linear_model(X_train.shape[1:],1)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    top_model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(X_Val, y_Val))\n",
    "    top_model.save_weights(top_model_weights_path)\n",
    "\n",
    "train_linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 50.0\n"
     ]
    }
   ],
   "source": [
    "#Test accuracy of linear model\n",
    "\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "\n",
    "top_model = load_linear_model(test_data.shape[1:])\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(top_model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 0s - loss: 7.6135 - acc: 0.4987 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 0s - loss: 8.0131 - acc: 0.4975 - val_loss: 7.8138 - val_acc: 0.5100\n"
     ]
    }
   ],
   "source": [
    "#Training model for linear claasifier with regularization\n",
    "from keras.regularizers import l2, activity_l2\n",
    "\n",
    "def load_linear_modelwithWreg(input_data_shape=1,train=0,reg=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(1, activation='sigmoid',W_regularizer=l2(reg)))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_linear_modelwithWreg(reg):\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    X_train, X_Val, y_train, y_Val=split_data(train_data,train_labels,0.2)\n",
    "    top_model = load_linear_modelwithWreg(X_train.shape[1:],1,reg)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    top_model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(X_Val, y_Val))\n",
    "    top_model.save_weights(top_model_weights_path)\n",
    "\n",
    "train_linear_modelwithWreg(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 50.0\n"
     ]
    }
   ],
   "source": [
    "#Test accuracy of linear model with regularization\n",
    "\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "top_model = load_linear_modelwithWreg(test_data.shape[1:])\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(top_model.metrics_names[1], scores[1]*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 0s - loss: 7.3096 - acc: 0.5140 - val_loss: 8.1965 - val_acc: 0.4860\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1965 - val_acc: 0.4860\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1965 - val_acc: 0.4860\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1965 - val_acc: 0.4860\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 0s - loss: 7.7501 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 0s - loss: 7.7500 - acc: 0.5140 - val_loss: 8.1964 - val_acc: 0.4860\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 0s - loss: 7.8078 - acc: 0.4820 - val_loss: 8.2392 - val_acc: 0.4860\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 0s - loss: 7.7925 - acc: 0.5140 - val_loss: 8.2386 - val_acc: 0.4860\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 0s - loss: 7.7919 - acc: 0.5140 - val_loss: 8.2380 - val_acc: 0.4860\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 0s - loss: 7.7913 - acc: 0.5140 - val_loss: 8.2374 - val_acc: 0.4860\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 0s - loss: 7.7908 - acc: 0.5140 - val_loss: 8.2368 - val_acc: 0.4860\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 0s - loss: 7.7902 - acc: 0.5140 - val_loss: 8.2363 - val_acc: 0.4860\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 0s - loss: 7.7896 - acc: 0.5140 - val_loss: 8.2357 - val_acc: 0.4860\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 0s - loss: 7.7891 - acc: 0.5140 - val_loss: 8.2352 - val_acc: 0.4860\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 0s - loss: 7.7885 - acc: 0.5140 - val_loss: 8.2346 - val_acc: 0.4860\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 0s - loss: 7.7880 - acc: 0.5140 - val_loss: 8.2341 - val_acc: 0.4860\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 0s - loss: 7.7875 - acc: 0.5140 - val_loss: 8.2336 - val_acc: 0.4860\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 0s - loss: 7.7869 - acc: 0.5140 - val_loss: 8.2330 - val_acc: 0.4860\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 0s - loss: 7.7864 - acc: 0.5140 - val_loss: 8.2325 - val_acc: 0.4860\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 0s - loss: 7.7859 - acc: 0.5140 - val_loss: 8.2320 - val_acc: 0.4860\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 0s - loss: 7.7854 - acc: 0.5140 - val_loss: 8.2315 - val_acc: 0.4860\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 0s - loss: 7.7849 - acc: 0.5140 - val_loss: 8.2310 - val_acc: 0.4860\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 0s - loss: 7.7844 - acc: 0.5140 - val_loss: 8.2305 - val_acc: 0.4860\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 0s - loss: 7.7839 - acc: 0.5140 - val_loss: 8.2300 - val_acc: 0.4860\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 0s - loss: 7.7834 - acc: 0.5140 - val_loss: 8.2296 - val_acc: 0.4860\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 0s - loss: 7.7830 - acc: 0.5140 - val_loss: 8.2291 - val_acc: 0.4860\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 0s - loss: 7.7825 - acc: 0.5140 - val_loss: 8.2286 - val_acc: 0.4860\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 0s - loss: 7.7820 - acc: 0.5140 - val_loss: 8.2282 - val_acc: 0.4860\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 0s - loss: 7.7816 - acc: 0.5140 - val_loss: 8.2277 - val_acc: 0.4860\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 0s - loss: 7.7811 - acc: 0.5140 - val_loss: 8.2273 - val_acc: 0.4860\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 0s - loss: 7.7807 - acc: 0.5140 - val_loss: 8.2268 - val_acc: 0.4860\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 0s - loss: 7.4929 - acc: 0.5020 - val_loss: 8.2789 - val_acc: 0.4860\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 0s - loss: 7.8315 - acc: 0.5140 - val_loss: 8.2767 - val_acc: 0.4860\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 0s - loss: 7.8293 - acc: 0.5140 - val_loss: 8.2746 - val_acc: 0.4860\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 0s - loss: 7.8272 - acc: 0.5140 - val_loss: 8.2725 - val_acc: 0.4860\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 0s - loss: 7.8252 - acc: 0.5140 - val_loss: 8.2705 - val_acc: 0.4860\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 0s - loss: 7.8232 - acc: 0.5140 - val_loss: 8.2685 - val_acc: 0.4860\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 0s - loss: 7.8212 - acc: 0.5140 - val_loss: 8.2666 - val_acc: 0.4860\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 0s - loss: 7.8193 - acc: 0.5140 - val_loss: 8.2647 - val_acc: 0.4860\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 0s - loss: 7.8175 - acc: 0.5140 - val_loss: 8.2629 - val_acc: 0.4860\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 0s - loss: 7.8157 - acc: 0.5140 - val_loss: 8.2611 - val_acc: 0.4860\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 0s - loss: 7.8139 - acc: 0.5140 - val_loss: 8.2594 - val_acc: 0.4860\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 0s - loss: 7.8122 - acc: 0.5140 - val_loss: 8.2577 - val_acc: 0.4860\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 0s - loss: 7.8105 - acc: 0.5140 - val_loss: 8.2560 - val_acc: 0.4860\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 0s - loss: 7.8089 - acc: 0.5140 - val_loss: 8.2545 - val_acc: 0.4860\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 0s - loss: 7.8073 - acc: 0.5140 - val_loss: 8.2529 - val_acc: 0.4860\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 0s - loss: 7.8058 - acc: 0.5140 - val_loss: 8.2514 - val_acc: 0.4860\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 0s - loss: 7.8043 - acc: 0.5140 - val_loss: 8.2499 - val_acc: 0.4860\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 0s - loss: 7.8029 - acc: 0.5140 - val_loss: 8.2485 - val_acc: 0.4860\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 0s - loss: 7.8014 - acc: 0.5140 - val_loss: 8.2471 - val_acc: 0.4860\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 0s - loss: 7.8000 - acc: 0.5140 - val_loss: 8.2457 - val_acc: 0.4860\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 0s - loss: 7.7987 - acc: 0.5140 - val_loss: 8.2444 - val_acc: 0.4860\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 0s - loss: 7.7974 - acc: 0.5140 - val_loss: 8.2431 - val_acc: 0.4860\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 0s - loss: 7.7961 - acc: 0.5140 - val_loss: 8.2418 - val_acc: 0.4860\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 0s - loss: 7.7949 - acc: 0.5140 - val_loss: 8.2406 - val_acc: 0.4860\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 0s - loss: 7.7936 - acc: 0.5140 - val_loss: 8.2394 - val_acc: 0.4860\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 0s - loss: 7.6197 - acc: 0.4940 - val_loss: 8.3184 - val_acc: 0.4860\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 0s - loss: 7.8699 - acc: 0.5140 - val_loss: 8.3137 - val_acc: 0.4860\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 0s - loss: 7.8652 - acc: 0.5140 - val_loss: 8.3091 - val_acc: 0.4860\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 0s - loss: 7.8607 - acc: 0.5140 - val_loss: 8.3047 - val_acc: 0.4860\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 0s - loss: 7.8564 - acc: 0.5140 - val_loss: 8.3005 - val_acc: 0.4860\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 0s - loss: 7.8522 - acc: 0.5140 - val_loss: 8.2964 - val_acc: 0.4860\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 0s - loss: 7.8482 - acc: 0.5140 - val_loss: 8.2925 - val_acc: 0.4860\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 0s - loss: 7.8444 - acc: 0.5140 - val_loss: 8.2888 - val_acc: 0.4860\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 0s - loss: 7.8407 - acc: 0.5140 - val_loss: 8.2851 - val_acc: 0.4860\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 0s - loss: 7.8372 - acc: 0.5140 - val_loss: 8.2817 - val_acc: 0.4860\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 0s - loss: 7.8337 - acc: 0.5140 - val_loss: 8.2783 - val_acc: 0.4860\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 0s - loss: 7.8305 - acc: 0.5140 - val_loss: 8.2751 - val_acc: 0.4860\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 0s - loss: 7.8273 - acc: 0.5140 - val_loss: 8.2720 - val_acc: 0.4860\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 0s - loss: 7.8243 - acc: 0.5140 - val_loss: 8.2690 - val_acc: 0.4860\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 0s - loss: 7.8213 - acc: 0.5140 - val_loss: 8.2662 - val_acc: 0.4860\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 0s - loss: 7.8185 - acc: 0.5140 - val_loss: 8.2634 - val_acc: 0.4860\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 0s - loss: 7.8158 - acc: 0.5140 - val_loss: 8.2608 - val_acc: 0.4860\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 0s - loss: 7.8132 - acc: 0.5140 - val_loss: 8.2582 - val_acc: 0.4860\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 0s - loss: 7.8107 - acc: 0.5140 - val_loss: 8.2558 - val_acc: 0.4860\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 0s - loss: 7.8083 - acc: 0.5140 - val_loss: 8.2534 - val_acc: 0.4860\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 0s - loss: 7.8060 - acc: 0.5140 - val_loss: 8.2512 - val_acc: 0.4860\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 0s - loss: 7.8038 - acc: 0.5140 - val_loss: 8.2490 - val_acc: 0.4860\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 0s - loss: 7.8017 - acc: 0.5140 - val_loss: 8.2469 - val_acc: 0.4860\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 0s - loss: 9.2212 - acc: 0.4260 - val_loss: 7.8853 - val_acc: 0.5140\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 0s - loss: 8.3357 - acc: 0.4860 - val_loss: 7.8833 - val_acc: 0.5140\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "500/500 [==============================] - 0s - loss: 7.0845 - acc: 0.5220 - val_loss: 8.3609 - val_acc: 0.4860\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 0s - loss: 7.9107 - acc: 0.5140 - val_loss: 8.3525 - val_acc: 0.4860\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 0s - loss: 7.9024 - acc: 0.5140 - val_loss: 8.3445 - val_acc: 0.4860\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 0s - loss: 7.8946 - acc: 0.5140 - val_loss: 8.3369 - val_acc: 0.4860\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 0s - loss: 7.8872 - acc: 0.5140 - val_loss: 8.3297 - val_acc: 0.4860\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 0s - loss: 7.8802 - acc: 0.5140 - val_loss: 8.3229 - val_acc: 0.4860\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 0s - loss: 7.8735 - acc: 0.5140 - val_loss: 8.3164 - val_acc: 0.4860\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 0s - loss: 7.8672 - acc: 0.5140 - val_loss: 8.3102 - val_acc: 0.4860\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 0s - loss: 7.8611 - acc: 0.5140 - val_loss: 8.3044 - val_acc: 0.4860\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 0s - loss: 7.8554 - acc: 0.5140 - val_loss: 8.2988 - val_acc: 0.4860\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 0s - loss: 7.8500 - acc: 0.5140 - val_loss: 8.2935 - val_acc: 0.4860\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 0s - loss: 7.8448 - acc: 0.5140 - val_loss: 8.2885 - val_acc: 0.4860\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 0s - loss: 7.8399 - acc: 0.5140 - val_loss: 8.2838 - val_acc: 0.4860\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 0s - loss: 7.8353 - acc: 0.5140 - val_loss: 8.2792 - val_acc: 0.4860\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 0s - loss: 7.8309 - acc: 0.5140 - val_loss: 8.2749 - val_acc: 0.4860\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 0s - loss: 7.8267 - acc: 0.5140 - val_loss: 8.2709 - val_acc: 0.4860\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 0s - loss: 7.8227 - acc: 0.5140 - val_loss: 8.2670 - val_acc: 0.4860\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 0s - loss: 7.8189 - acc: 0.5140 - val_loss: 8.2633 - val_acc: 0.4860\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 0s - loss: 7.8153 - acc: 0.5140 - val_loss: 8.2598 - val_acc: 0.4860\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 0s - loss: 7.8119 - acc: 0.5140 - val_loss: 8.2565 - val_acc: 0.4860\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 0s - loss: 7.8087 - acc: 0.5140 - val_loss: 8.2534 - val_acc: 0.4860\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 0s - loss: 7.8056 - acc: 0.5140 - val_loss: 8.2504 - val_acc: 0.4860\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 0s - loss: 7.8027 - acc: 0.5140 - val_loss: 8.2476 - val_acc: 0.4860\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 0s - loss: 7.7408 - acc: 0.5060 - val_loss: 8.2477 - val_acc: 0.4860\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 0s - loss: 7.8001 - acc: 0.5140 - val_loss: 8.2450 - val_acc: 0.4860\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGHCAYAAACJeOnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wf7CYYjCdZvNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bc42685be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting test accuracy as a function of regularization parameter\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "\n",
    "test_acc=[]\n",
    "reg_values=[]\n",
    "for reg in range(1, 100, 20):\n",
    "    train_linear_modelwithWreg(reg/1000)\n",
    "    top_model = load_linear_modelwithWreg(test_data.shape[1:])\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "    reg_values.append(reg/1000)\n",
    "    test_acc.append(scores[1]*100)\n",
    "    \n",
    "plt.plot(reg_values,test_acc,'r',label = \"Test accuracy\") \n",
    "plt.xlabel(\"Regularization parameter\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Sensitivity of the model\")\n",
    "#plt.text(100, max(train_acc[2])-max(train_acc[2]/10), 'Test accuracy' ,color = 'green')\n",
    "#plt.text(100, max(train_acc[2])-max(train_acc[2]/8), 'Train accuracy' ,color = 'red')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 2s - loss: 0.9193 - acc: 0.5625 - val_loss: 0.6944 - val_acc: 0.5100\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 2s - loss: 0.6880 - acc: 0.5600 - val_loss: 0.6610 - val_acc: 0.5300\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 2s - loss: 0.6483 - acc: 0.6175 - val_loss: 0.6654 - val_acc: 0.5100\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 2s - loss: 0.6152 - acc: 0.6550 - val_loss: 0.6096 - val_acc: 0.6250\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 2s - loss: 0.6189 - acc: 0.6450 - val_loss: 0.5597 - val_acc: 0.7500\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 2s - loss: 0.5349 - acc: 0.7500 - val_loss: 0.4959 - val_acc: 0.8100\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 2s - loss: 0.5470 - acc: 0.7263 - val_loss: 0.5122 - val_acc: 0.8000\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 2s - loss: 0.5187 - acc: 0.7587 - val_loss: 0.4625 - val_acc: 0.8450\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 2s - loss: 0.5868 - acc: 0.6950 - val_loss: 0.4950 - val_acc: 0.8350\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 2s - loss: 0.5977 - acc: 0.6675 - val_loss: 0.6311 - val_acc: 0.5150\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 2s - loss: 0.5372 - acc: 0.7550 - val_loss: 0.5105 - val_acc: 0.7600\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 2s - loss: 0.5276 - acc: 0.7350 - val_loss: 0.7669 - val_acc: 0.5250\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 2s - loss: 0.5357 - acc: 0.7300 - val_loss: 0.8567 - val_acc: 0.5150\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 2s - loss: 0.5048 - acc: 0.7512 - val_loss: 0.4562 - val_acc: 0.8600\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 2s - loss: 0.4559 - acc: 0.7825 - val_loss: 0.4670 - val_acc: 0.8150\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 2s - loss: 0.4986 - acc: 0.7675 - val_loss: 0.4746 - val_acc: 0.8100\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 2s - loss: 0.3988 - acc: 0.8250 - val_loss: 0.4127 - val_acc: 0.8050\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 2s - loss: 0.4027 - acc: 0.8113 - val_loss: 0.4840 - val_acc: 0.7650\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 2s - loss: 0.4561 - acc: 0.7800 - val_loss: 0.6501 - val_acc: 0.6050\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 2s - loss: 0.4148 - acc: 0.8113 - val_loss: 0.4272 - val_acc: 0.8050\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 2s - loss: 0.3968 - acc: 0.8200 - val_loss: 0.3817 - val_acc: 0.8600\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 2s - loss: 0.3565 - acc: 0.8325 - val_loss: 0.3551 - val_acc: 0.8550\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 2s - loss: 0.3595 - acc: 0.8325 - val_loss: 0.8893 - val_acc: 0.5800\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 2s - loss: 0.3590 - acc: 0.8463 - val_loss: 0.5101 - val_acc: 0.7800\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 2s - loss: 0.3446 - acc: 0.8562 - val_loss: 0.5199 - val_acc: 0.7250\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 2s - loss: 0.3746 - acc: 0.8287 - val_loss: 0.4737 - val_acc: 0.7500\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 2s - loss: 0.3718 - acc: 0.8313 - val_loss: 0.3384 - val_acc: 0.8450\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 2s - loss: 0.3128 - acc: 0.8712 - val_loss: 0.3835 - val_acc: 0.8250\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 2s - loss: 0.3238 - acc: 0.8625 - val_loss: 0.3799 - val_acc: 0.8200\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 2s - loss: 0.2566 - acc: 0.9062 - val_loss: 0.3436 - val_acc: 0.8450\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 2s - loss: 0.3503 - acc: 0.8475 - val_loss: 0.3396 - val_acc: 0.8500\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 2s - loss: 0.3212 - acc: 0.8575 - val_loss: 0.3391 - val_acc: 0.8400\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 2s - loss: 0.2723 - acc: 0.8975 - val_loss: 0.3865 - val_acc: 0.8250\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 2s - loss: 0.3958 - acc: 0.8025 - val_loss: 0.4374 - val_acc: 0.8250\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 2s - loss: 0.2873 - acc: 0.8900 - val_loss: 0.3557 - val_acc: 0.8500\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 2s - loss: 0.2610 - acc: 0.8975 - val_loss: 0.3437 - val_acc: 0.8400\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 2s - loss: 0.2785 - acc: 0.8850 - val_loss: 0.4116 - val_acc: 0.8400\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 2s - loss: 0.2992 - acc: 0.8762 - val_loss: 0.4016 - val_acc: 0.8250\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 2s - loss: 0.3188 - acc: 0.8700 - val_loss: 0.4897 - val_acc: 0.7450\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 2s - loss: 0.2516 - acc: 0.8938 - val_loss: 0.3623 - val_acc: 0.8400\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 2s - loss: 0.1926 - acc: 0.9300 - val_loss: 0.5053 - val_acc: 0.7950\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 2s - loss: 0.1862 - acc: 0.9387 - val_loss: 0.3688 - val_acc: 0.8400\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 2s - loss: 0.2200 - acc: 0.9163 - val_loss: 0.3566 - val_acc: 0.8350\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 2s - loss: 0.2094 - acc: 0.9163 - val_loss: 0.3650 - val_acc: 0.8500\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 2s - loss: 0.1738 - acc: 0.9275 - val_loss: 0.3747 - val_acc: 0.8350\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 2s - loss: 0.2151 - acc: 0.9225 - val_loss: 0.3552 - val_acc: 0.8500\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 2s - loss: 0.2481 - acc: 0.9000 - val_loss: 0.3549 - val_acc: 0.8300\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 2s - loss: 0.4031 - acc: 0.8538 - val_loss: 0.4022 - val_acc: 0.8350\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 2s - loss: 0.3181 - acc: 0.8712 - val_loss: 0.5955 - val_acc: 0.7000\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 2s - loss: 0.3004 - acc: 0.8738 - val_loss: 0.4693 - val_acc: 0.7700\n"
     ]
    }
   ],
   "source": [
    "# Training model with two layers and relu activation\n",
    "\n",
    "nb_epoch=50\n",
    "def load_relu_model(input_data_shape=1,train=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_relu_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    X_train, X_Val, y_train, y_Val=split_data(train_data,train_labels,0.2)\n",
    "    top_model = load_relu_model(X_train.shape[1:],1)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    top_model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(X_Val, y_Val))\n",
    "    top_model.save_weights(top_model_weights_path)\n",
    "    \n",
    "train_relu_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 77.9999998411\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on testing data for two layers and relu activation\n",
    "\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "top_model = load_relu_model(test_data.shape[1:])\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(top_model.metrics_names[1], scores[1]*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 0.9797 - acc: 0.5225 - val_loss: 0.6625 - val_acc: 0.7100\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.6554 - acc: 0.6075 - val_loss: 0.6313 - val_acc: 0.7750\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.6238 - acc: 0.6500 - val_loss: 0.5908 - val_acc: 0.7850\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.5791 - acc: 0.7112 - val_loss: 0.5896 - val_acc: 0.6700\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.5705 - acc: 0.7000 - val_loss: 0.6172 - val_acc: 0.6050\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.5641 - acc: 0.7300 - val_loss: 0.6757 - val_acc: 0.5450\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.5631 - acc: 0.7050 - val_loss: 0.4976 - val_acc: 0.7600\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.5321 - acc: 0.7462 - val_loss: 0.5462 - val_acc: 0.7550\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.5690 - acc: 0.7338 - val_loss: 0.4884 - val_acc: 0.7600\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.4947 - acc: 0.7625 - val_loss: 0.4276 - val_acc: 0.8150\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.4501 - acc: 0.7913 - val_loss: 0.5214 - val_acc: 0.7050\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.4455 - acc: 0.7762 - val_loss: 0.4092 - val_acc: 0.8050\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4513 - acc: 0.7900 - val_loss: 0.4411 - val_acc: 0.7850\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.3921 - acc: 0.8375 - val_loss: 0.6564 - val_acc: 0.6550\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.4399 - acc: 0.7987 - val_loss: 0.3727 - val_acc: 0.8650\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.3835 - acc: 0.8375 - val_loss: 0.3861 - val_acc: 0.8550\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.3442 - acc: 0.8412 - val_loss: 0.6794 - val_acc: 0.6550\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.4054 - acc: 0.8137 - val_loss: 0.3893 - val_acc: 0.8550\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.4115 - acc: 0.7963 - val_loss: 0.3736 - val_acc: 0.8350\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.3922 - acc: 0.8250 - val_loss: 0.3720 - val_acc: 0.8550\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.3983 - acc: 0.8013 - val_loss: 0.3535 - val_acc: 0.8650\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.2680 - acc: 0.8975 - val_loss: 0.3573 - val_acc: 0.8250\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.3016 - acc: 0.8738 - val_loss: 0.3497 - val_acc: 0.8350\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.4068 - acc: 0.8275 - val_loss: 0.3898 - val_acc: 0.8400\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.2982 - acc: 0.8837 - val_loss: 0.3657 - val_acc: 0.8550\n"
     ]
    }
   ],
   "source": [
    "# Training model with two layers and relu activation\n",
    "def load_Leakyrelu_model(input_data_shape=1,train=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(256))\n",
    "    top_model.add(LeakyReLU(alpha=.001)) # adding leakyrelu activation\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_Leakyrelu_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    X_train, X_Val, y_train, y_Val=split_data(train_data,train_labels,0.2)\n",
    "    top_model = load_Leakyrelu_model(X_train.shape[1:],1)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    top_model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(X_Val, y_Val))\n",
    "    top_model.save_weights(top_model_weights_path)\n",
    "    \n",
    "train_Leakyrelu_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 82.6666663488\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on testing data for two layers and Leakyrelu activation\n",
    "\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "top_model = load_Leakyrelu_model(test_data.shape[1:])\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(top_model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 1.1210 - acc: 0.4975 - val_loss: 0.6562 - val_acc: 0.6000\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.8695 - acc: 0.5000 - val_loss: 0.7855 - val_acc: 0.5100\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.7765 - acc: 0.5675 - val_loss: 0.5661 - val_acc: 0.7200\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.7282 - acc: 0.6163 - val_loss: 0.5766 - val_acc: 0.6450\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.5985 - acc: 0.6925 - val_loss: 0.5572 - val_acc: 0.6500\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.7930 - acc: 0.6050 - val_loss: 0.5692 - val_acc: 0.7000\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.6515 - acc: 0.6737 - val_loss: 0.5055 - val_acc: 0.7550\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.6175 - acc: 0.6900 - val_loss: 0.4783 - val_acc: 0.8250\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.5268 - acc: 0.7450 - val_loss: 0.4151 - val_acc: 0.8300\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.5880 - acc: 0.7175 - val_loss: 0.6565 - val_acc: 0.6350\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.5539 - acc: 0.7362 - val_loss: 0.5742 - val_acc: 0.6650\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.4652 - acc: 0.7787 - val_loss: 0.4913 - val_acc: 0.7650\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4194 - acc: 0.8038 - val_loss: 0.7103 - val_acc: 0.5700\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.4364 - acc: 0.7925 - val_loss: 0.3710 - val_acc: 0.8600\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.4511 - acc: 0.7925 - val_loss: 0.5403 - val_acc: 0.7150\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.4728 - acc: 0.7787 - val_loss: 0.8863 - val_acc: 0.6050\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.4535 - acc: 0.7925 - val_loss: 0.5932 - val_acc: 0.6950\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.4051 - acc: 0.8025 - val_loss: 0.4171 - val_acc: 0.8300\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.3576 - acc: 0.8438 - val_loss: 0.4314 - val_acc: 0.7850\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.3142 - acc: 0.8650 - val_loss: 0.7478 - val_acc: 0.6650\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.3960 - acc: 0.8237 - val_loss: 0.4036 - val_acc: 0.8200\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.3291 - acc: 0.8588 - val_loss: 0.3776 - val_acc: 0.8500\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.2712 - acc: 0.8888 - val_loss: 0.4828 - val_acc: 0.8250\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.3298 - acc: 0.8712 - val_loss: 0.4726 - val_acc: 0.8250\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.3420 - acc: 0.8488 - val_loss: 0.3409 - val_acc: 0.8400\n"
     ]
    }
   ],
   "source": [
    "# Training model with two layers and tanh activation\n",
    "def load_tanh_model(input_data_shape=1,train=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(256,activation='tanh'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_tanh_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    X_train, X_Val, y_train, y_Val=split_data(train_data,train_labels,0.2)\n",
    "    top_model = load_tanh_model(X_train.shape[1:],1)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    top_model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(X_Val, y_Val))\n",
    "    top_model.save_weights(top_model_weights_path)\n",
    "    \n",
    "train_tanh_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 80.6666667461\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on testing data for two layers and tanh activation\n",
    "\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "top_model = load_tanh_model(test_data.shape[1:])\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(top_model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 0.8955 - acc: 0.5487 - val_loss: 0.7248 - val_acc: 0.5100\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.6659 - acc: 0.5988 - val_loss: 0.6305 - val_acc: 0.5850\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.6384 - acc: 0.6250 - val_loss: 0.5985 - val_acc: 0.7300\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.6301 - acc: 0.6613 - val_loss: 0.5557 - val_acc: 0.7950\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.5940 - acc: 0.6775 - val_loss: 0.5923 - val_acc: 0.6400\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.5840 - acc: 0.6925 - val_loss: 0.5096 - val_acc: 0.7950\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.5483 - acc: 0.7188 - val_loss: 0.5841 - val_acc: 0.6350\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.4959 - acc: 0.7675 - val_loss: 0.4525 - val_acc: 0.8450\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.5988 - acc: 0.7050 - val_loss: 0.5796 - val_acc: 0.6350\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.4997 - acc: 0.7762 - val_loss: 0.5376 - val_acc: 0.6700\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.4393 - acc: 0.8113 - val_loss: 0.4152 - val_acc: 0.8750\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.4847 - acc: 0.7625 - val_loss: 0.4704 - val_acc: 0.7700\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4729 - acc: 0.7725 - val_loss: 0.4341 - val_acc: 0.8150\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.4498 - acc: 0.8000 - val_loss: 0.4480 - val_acc: 0.7950\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.4790 - acc: 0.7750 - val_loss: 0.4214 - val_acc: 0.8000\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.4162 - acc: 0.8150 - val_loss: 0.3532 - val_acc: 0.8950\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.4512 - acc: 0.7575 - val_loss: 0.5348 - val_acc: 0.7000\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.4513 - acc: 0.8013 - val_loss: 0.9146 - val_acc: 0.5450\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.3435 - acc: 0.8625 - val_loss: 0.4059 - val_acc: 0.8450\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.3867 - acc: 0.8250 - val_loss: 0.3520 - val_acc: 0.8350\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.4122 - acc: 0.8237 - val_loss: 0.3427 - val_acc: 0.8600\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.3335 - acc: 0.8450 - val_loss: 0.3300 - val_acc: 0.8550\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.3056 - acc: 0.8750 - val_loss: 0.3139 - val_acc: 0.8650\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.3847 - acc: 0.8325 - val_loss: 0.8007 - val_acc: 0.5500\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.2872 - acc: 0.8850 - val_loss: 0.3434 - val_acc: 0.9000\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 0.9488 - acc: 0.5125 - val_loss: 0.6820 - val_acc: 0.4900\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.6841 - acc: 0.5375 - val_loss: 0.6561 - val_acc: 0.5400\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.6585 - acc: 0.6050 - val_loss: 0.6180 - val_acc: 0.8500\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.6283 - acc: 0.6562 - val_loss: 0.5744 - val_acc: 0.8450\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.6039 - acc: 0.7037 - val_loss: 0.5413 - val_acc: 0.8550\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.5865 - acc: 0.6987 - val_loss: 0.5868 - val_acc: 0.6150\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.5554 - acc: 0.7325 - val_loss: 0.5057 - val_acc: 0.8250\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.5804 - acc: 0.7075 - val_loss: 0.5168 - val_acc: 0.8750\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.5391 - acc: 0.7425 - val_loss: 0.4759 - val_acc: 0.8500\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.4785 - acc: 0.7838 - val_loss: 0.5511 - val_acc: 0.6050\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.5127 - acc: 0.7488 - val_loss: 0.6084 - val_acc: 0.6100\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.5371 - acc: 0.7437 - val_loss: 0.4913 - val_acc: 0.8250\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4622 - acc: 0.7863 - val_loss: 0.3525 - val_acc: 0.9000\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.5208 - acc: 0.7350 - val_loss: 0.4609 - val_acc: 0.7950\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.4362 - acc: 0.8113 - val_loss: 1.1441 - val_acc: 0.4900\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.4483 - acc: 0.8038 - val_loss: 0.4292 - val_acc: 0.7850\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.3616 - acc: 0.8450 - val_loss: 0.3995 - val_acc: 0.8150\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.3973 - acc: 0.8225 - val_loss: 0.3335 - val_acc: 0.8850\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.3989 - acc: 0.8062 - val_loss: 0.5431 - val_acc: 0.7150\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.4239 - acc: 0.7963 - val_loss: 0.4215 - val_acc: 0.8050\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.3555 - acc: 0.8625 - val_loss: 0.2862 - val_acc: 0.9050\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.3690 - acc: 0.8500 - val_loss: 0.3157 - val_acc: 0.9100\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.3630 - acc: 0.8463 - val_loss: 0.2897 - val_acc: 0.9150\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.3610 - acc: 0.8325 - val_loss: 0.4367 - val_acc: 0.7700\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.3389 - acc: 0.8700 - val_loss: 0.3012 - val_acc: 0.9100\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 0.9007 - acc: 0.5400 - val_loss: 0.6887 - val_acc: 0.5250\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.6669 - acc: 0.5737 - val_loss: 0.6409 - val_acc: 0.6450\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.6214 - acc: 0.6737 - val_loss: 0.5952 - val_acc: 0.7600\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.6031 - acc: 0.6825 - val_loss: 0.6030 - val_acc: 0.6400\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.6001 - acc: 0.6650 - val_loss: 0.5715 - val_acc: 0.7300\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.5556 - acc: 0.7300 - val_loss: 0.5621 - val_acc: 0.7200\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.5148 - acc: 0.7425 - val_loss: 0.5311 - val_acc: 0.7200\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.5375 - acc: 0.7200 - val_loss: 0.5588 - val_acc: 0.6050\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.4791 - acc: 0.7775 - val_loss: 0.4892 - val_acc: 0.7400\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.5301 - acc: 0.7437 - val_loss: 0.6520 - val_acc: 0.6150\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.4593 - acc: 0.7775 - val_loss: 0.9360 - val_acc: 0.5250\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.4558 - acc: 0.7863 - val_loss: 0.4492 - val_acc: 0.7900\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4381 - acc: 0.7888 - val_loss: 0.8503 - val_acc: 0.5750\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.4440 - acc: 0.8025 - val_loss: 0.4067 - val_acc: 0.8250\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.4188 - acc: 0.8050 - val_loss: 0.3819 - val_acc: 0.8450\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.3922 - acc: 0.8237 - val_loss: 0.7835 - val_acc: 0.5550\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.3772 - acc: 0.8350 - val_loss: 0.5226 - val_acc: 0.7150\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.3264 - acc: 0.8638 - val_loss: 0.3385 - val_acc: 0.8550\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.4285 - acc: 0.8237 - val_loss: 0.3525 - val_acc: 0.8550\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.3346 - acc: 0.8525 - val_loss: 0.6021 - val_acc: 0.6400\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.4454 - acc: 0.7937 - val_loss: 0.5115 - val_acc: 0.6800\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.3669 - acc: 0.8500 - val_loss: 0.3541 - val_acc: 0.8300\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.3430 - acc: 0.8500 - val_loss: 0.4541 - val_acc: 0.7800\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.3098 - acc: 0.8675 - val_loss: 0.4828 - val_acc: 0.7500\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.3457 - acc: 0.8562 - val_loss: 0.5612 - val_acc: 0.6500\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 0.9038 - acc: 0.5175 - val_loss: 0.6708 - val_acc: 0.6650\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.6646 - acc: 0.6050 - val_loss: 0.6470 - val_acc: 0.6450\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.6411 - acc: 0.6350 - val_loss: 0.7989 - val_acc: 0.4700\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.6070 - acc: 0.6675 - val_loss: 0.5711 - val_acc: 0.6950\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.5924 - acc: 0.6875 - val_loss: 0.5363 - val_acc: 0.7850\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.5797 - acc: 0.7100 - val_loss: 0.5268 - val_acc: 0.7550\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.5581 - acc: 0.7087 - val_loss: 0.6703 - val_acc: 0.5000\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.5091 - acc: 0.7500 - val_loss: 0.4753 - val_acc: 0.7900\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.5090 - acc: 0.7550 - val_loss: 0.5893 - val_acc: 0.6350\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.4992 - acc: 0.7600 - val_loss: 0.6141 - val_acc: 0.6050\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.4732 - acc: 0.7888 - val_loss: 0.4163 - val_acc: 0.8200\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.4563 - acc: 0.8275 - val_loss: 0.4956 - val_acc: 0.7750\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4651 - acc: 0.7900 - val_loss: 0.4458 - val_acc: 0.7950\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.3793 - acc: 0.8387 - val_loss: 0.3706 - val_acc: 0.8250\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.5019 - acc: 0.7625 - val_loss: 0.4283 - val_acc: 0.8100\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.4131 - acc: 0.8062 - val_loss: 0.4639 - val_acc: 0.7700\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.4009 - acc: 0.8163 - val_loss: 0.5546 - val_acc: 0.7000\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.4427 - acc: 0.7812 - val_loss: 0.4328 - val_acc: 0.7750\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.3550 - acc: 0.8488 - val_loss: 0.3401 - val_acc: 0.8500\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.3929 - acc: 0.8463 - val_loss: 0.4553 - val_acc: 0.7750\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.4194 - acc: 0.8075 - val_loss: 0.3635 - val_acc: 0.8500\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.3323 - acc: 0.8575 - val_loss: 0.5890 - val_acc: 0.6900\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.3907 - acc: 0.8050 - val_loss: 0.7015 - val_acc: 0.6350\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.3818 - acc: 0.8087 - val_loss: 0.4010 - val_acc: 0.8250\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.3145 - acc: 0.8650 - val_loss: 0.3914 - val_acc: 0.8000\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 2s - loss: 0.9471 - acc: 0.5325 - val_loss: 0.6652 - val_acc: 0.6800\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 2s - loss: 0.6736 - acc: 0.5813 - val_loss: 0.6572 - val_acc: 0.5300\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 2s - loss: 0.6220 - acc: 0.6663 - val_loss: 0.6662 - val_acc: 0.5550\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 2s - loss: 0.6292 - acc: 0.6350 - val_loss: 0.5817 - val_acc: 0.7700\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 2s - loss: 0.5904 - acc: 0.6837 - val_loss: 0.5862 - val_acc: 0.6700\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 2s - loss: 0.5595 - acc: 0.7175 - val_loss: 0.5253 - val_acc: 0.7950\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 2s - loss: 0.5394 - acc: 0.7213 - val_loss: 0.5000 - val_acc: 0.7950\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 2s - loss: 0.4961 - acc: 0.7462 - val_loss: 0.5037 - val_acc: 0.7600\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 2s - loss: 0.5002 - acc: 0.7450 - val_loss: 0.5454 - val_acc: 0.7150\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 2s - loss: 0.3982 - acc: 0.8325 - val_loss: 0.4427 - val_acc: 0.8050\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 2s - loss: 0.5381 - acc: 0.7137 - val_loss: 0.5012 - val_acc: 0.7650\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 2s - loss: 0.4402 - acc: 0.8113 - val_loss: 0.5730 - val_acc: 0.6850\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 2s - loss: 0.4207 - acc: 0.8013 - val_loss: 0.4631 - val_acc: 0.7800\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 2s - loss: 0.4595 - acc: 0.7750 - val_loss: 0.5174 - val_acc: 0.7200\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 2s - loss: 0.3613 - acc: 0.8425 - val_loss: 0.4819 - val_acc: 0.7600\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 2s - loss: 0.3856 - acc: 0.8137 - val_loss: 0.4291 - val_acc: 0.8000\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 2s - loss: 0.4267 - acc: 0.8125 - val_loss: 0.4251 - val_acc: 0.8050\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 2s - loss: 0.3457 - acc: 0.8463 - val_loss: 0.8556 - val_acc: 0.5950\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 2s - loss: 0.4529 - acc: 0.7725 - val_loss: 0.4833 - val_acc: 0.7750\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 2s - loss: 0.3194 - acc: 0.8538 - val_loss: 0.4814 - val_acc: 0.7900\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 2s - loss: 0.4211 - acc: 0.8013 - val_loss: 0.4115 - val_acc: 0.8200\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 2s - loss: 0.3388 - acc: 0.8562 - val_loss: 0.4174 - val_acc: 0.8200\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 2s - loss: 0.3427 - acc: 0.8513 - val_loss: 0.5381 - val_acc: 0.7400\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 2s - loss: 0.2903 - acc: 0.8600 - val_loss: 0.4047 - val_acc: 0.8050\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 2s - loss: 0.2614 - acc: 0.8975 - val_loss: 0.5582 - val_acc: 0.7350\n"
     ]
    }
   ],
   "source": [
    "# Training Cross validation with two layers and relu activation\n",
    "def load_relu_model(input_data_shape=1,train=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(256,activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid'))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_relu_model_crossval():\n",
    "    \n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    cv = cross_validation.KFold(len(train_data), n_folds=5, shuffle=True)\n",
    "    i = 0\n",
    "    for train_indices, test_indices in cv:\n",
    "        i = i + 1  \n",
    "        top_model = load_relu_model(train_data[train_indices].shape[1:],1)\n",
    "        top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        top_model.fit(train_data[train_indices], train_labels[train_indices],\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(train_data[test_indices], train_labels[test_indices]))\n",
    "    top_model.save_weights(top_model_weights_path)\n",
    "    \n",
    "train_relu_model_crossval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 75.9999997616\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "top_model = load_relu_model(test_data.shape[1:])\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(top_model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training model for relu clasifier with regularization\n",
    "from keras.regularizers import l2, activity_l2\n",
    "\n",
    "def load_relu_modelwithWreg(input_data_shape=1,train=0,reg=0):\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=input_data_shape))\n",
    "    top_model.add(Dense(256,activation='relu',W_regularizer=l2(reg)))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(1, activation='sigmoid',W_regularizer=l2(reg)))\n",
    "    if train==0:\n",
    "        top_model.load_weights(top_model_weights_path)    \n",
    "    return(top_model)\n",
    "\n",
    "def train_relu_modelwithWreg(reg):\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    X_train, X_Val, y_train, y_Val=split_data(train_data,train_labels,0.2)\n",
    "    top_model = load_relu_modelwithWreg(X_train.shape[1:],1,reg)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    top_model.fit(X_train, y_train,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(X_Val, y_Val))\n",
    "    top_model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 6s - loss: 1.4738 - acc: 0.5262 - val_loss: 1.2396 - val_acc: 0.5100\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 5s - loss: 1.1696 - acc: 0.6000 - val_loss: 1.1955 - val_acc: 0.4900\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 5s - loss: 1.1499 - acc: 0.6550 - val_loss: 1.1253 - val_acc: 0.5400\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 5s - loss: 1.1253 - acc: 0.6438 - val_loss: 1.0891 - val_acc: 0.6700\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 5s - loss: 1.0974 - acc: 0.6938 - val_loss: 1.0621 - val_acc: 0.8050\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 5s - loss: 1.0980 - acc: 0.7112 - val_loss: 1.0125 - val_acc: 0.8400\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 5s - loss: 1.0295 - acc: 0.7538 - val_loss: 1.0528 - val_acc: 0.6650\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 5s - loss: 1.0341 - acc: 0.7338 - val_loss: 1.0115 - val_acc: 0.8050\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 7s - loss: 1.0134 - acc: 0.7662 - val_loss: 0.9358 - val_acc: 0.8300\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 5s - loss: 1.0337 - acc: 0.7450 - val_loss: 0.9395 - val_acc: 0.8300\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 6s - loss: 0.9716 - acc: 0.7650 - val_loss: 0.9428 - val_acc: 0.8500\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 5s - loss: 0.9899 - acc: 0.7725 - val_loss: 0.9485 - val_acc: 0.7950\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 5s - loss: 0.9136 - acc: 0.8175 - val_loss: 1.0233 - val_acc: 0.7400\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 5s - loss: 0.9516 - acc: 0.7863 - val_loss: 0.9008 - val_acc: 0.8300\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 5s - loss: 0.9398 - acc: 0.8075 - val_loss: 0.8839 - val_acc: 0.8600\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 5s - loss: 0.9113 - acc: 0.8250 - val_loss: 0.9412 - val_acc: 0.7850\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 5s - loss: 0.9244 - acc: 0.8000 - val_loss: 0.9662 - val_acc: 0.7750\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 5s - loss: 0.9023 - acc: 0.8125 - val_loss: 0.8714 - val_acc: 0.8250\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 5s - loss: 0.8737 - acc: 0.8362 - val_loss: 0.8988 - val_acc: 0.8250\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 5s - loss: 0.8849 - acc: 0.8275 - val_loss: 0.9656 - val_acc: 0.8050\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 5s - loss: 0.8344 - acc: 0.8738 - val_loss: 1.0839 - val_acc: 0.7200\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 5s - loss: 0.9508 - acc: 0.7775 - val_loss: 0.8885 - val_acc: 0.8200\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 5s - loss: 0.8122 - acc: 0.8788 - val_loss: 0.9680 - val_acc: 0.7700\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 5s - loss: 0.8240 - acc: 0.8600 - val_loss: 0.9365 - val_acc: 0.8300\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 5s - loss: 0.8002 - acc: 0.8900 - val_loss: 0.8550 - val_acc: 0.8650\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 6s - loss: 11.4910 - acc: 0.5400 - val_loss: 11.1264 - val_acc: 0.6000\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 5s - loss: 11.0361 - acc: 0.5925 - val_loss: 10.8666 - val_acc: 0.7750\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 6s - loss: 10.7906 - acc: 0.6337 - val_loss: 10.6259 - val_acc: 0.7350\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 5s - loss: 10.6094 - acc: 0.5913 - val_loss: 10.4662 - val_acc: 0.5650\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 6s - loss: 10.3118 - acc: 0.7125 - val_loss: 10.2141 - val_acc: 0.6550\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 5s - loss: 10.0947 - acc: 0.7075 - val_loss: 9.9513 - val_acc: 0.8100\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 5s - loss: 9.9141 - acc: 0.7000 - val_loss: 9.7392 - val_acc: 0.8200\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 5s - loss: 9.6757 - acc: 0.7275 - val_loss: 9.6583 - val_acc: 0.5500\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 5s - loss: 9.5416 - acc: 0.7000 - val_loss: 9.4225 - val_acc: 0.6800\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 5s - loss: 9.2912 - acc: 0.7437 - val_loss: 9.2103 - val_acc: 0.6850\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 5s - loss: 9.0890 - acc: 0.7787 - val_loss: 8.9383 - val_acc: 0.8050\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 5s - loss: 8.9260 - acc: 0.7538 - val_loss: 8.8308 - val_acc: 0.7200\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 5s - loss: 8.7094 - acc: 0.7737 - val_loss: 8.5653 - val_acc: 0.8500\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 5s - loss: 8.5051 - acc: 0.8137 - val_loss: 8.3907 - val_acc: 0.8550\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 5s - loss: 8.4105 - acc: 0.7587 - val_loss: 8.2550 - val_acc: 0.8500\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 5s - loss: 8.1702 - acc: 0.8050 - val_loss: 8.0433 - val_acc: 0.8350\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 5s - loss: 8.0298 - acc: 0.7963 - val_loss: 8.0154 - val_acc: 0.7500\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 7s - loss: 7.8238 - acc: 0.8375 - val_loss: 7.8502 - val_acc: 0.7700\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 7s - loss: 7.7382 - acc: 0.7900 - val_loss: 7.5926 - val_acc: 0.8650\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 5s - loss: 7.5004 - acc: 0.8525 - val_loss: 7.5093 - val_acc: 0.7750\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 6s - loss: 7.3838 - acc: 0.8113 - val_loss: 7.2812 - val_acc: 0.8600\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 6s - loss: 7.1998 - acc: 0.8400 - val_loss: 7.2531 - val_acc: 0.7450\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 6s - loss: 7.0406 - acc: 0.8525 - val_loss: 7.0102 - val_acc: 0.8150\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 6s - loss: 6.9484 - acc: 0.8313 - val_loss: 6.9781 - val_acc: 0.7500\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 6s - loss: 6.8369 - acc: 0.8125 - val_loss: 6.7740 - val_acc: 0.8000\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 6s - loss: 21.4493 - acc: 0.5150 - val_loss: 20.6936 - val_acc: 0.5450\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 5s - loss: 20.2886 - acc: 0.6112 - val_loss: 19.8618 - val_acc: 0.6350\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 6s - loss: 19.5120 - acc: 0.6150 - val_loss: 19.0643 - val_acc: 0.7450\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 7s - loss: 18.7122 - acc: 0.6575 - val_loss: 18.3755 - val_acc: 0.5300\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 5s - loss: 17.9745 - acc: 0.6500 - val_loss: 17.5739 - val_acc: 0.7300\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 5s - loss: 17.2560 - acc: 0.6950 - val_loss: 16.8440 - val_acc: 0.8000\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 9s - loss: 16.5690 - acc: 0.7200 - val_loss: 16.3339 - val_acc: 0.5500\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 10s - loss: 15.9216 - acc: 0.7100 - val_loss: 15.5642 - val_acc: 0.7750\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 10s - loss: 15.2857 - acc: 0.7112 - val_loss: 14.9720 - val_acc: 0.7250\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 10s - loss: 14.6951 - acc: 0.7425 - val_loss: 14.3323 - val_acc: 0.8000\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 11s - loss: 14.0781 - acc: 0.7787 - val_loss: 13.8243 - val_acc: 0.7600\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 12s - loss: 13.5304 - acc: 0.7937 - val_loss: 13.7498 - val_acc: 0.5000\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 11s - loss: 13.0440 - acc: 0.7650 - val_loss: 12.7063 - val_acc: 0.8150\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 11s - loss: 12.4913 - acc: 0.7937 - val_loss: 12.1924 - val_acc: 0.8600\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 11s - loss: 11.9553 - acc: 0.8263 - val_loss: 11.8231 - val_acc: 0.7450\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 10s - loss: 11.5983 - acc: 0.7512 - val_loss: 11.3000 - val_acc: 0.8550\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 11s - loss: 11.0822 - acc: 0.8250 - val_loss: 10.8274 - val_acc: 0.8150\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 10s - loss: 10.7856 - acc: 0.7213 - val_loss: 10.5820 - val_acc: 0.6650\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 7s - loss: 10.2863 - acc: 0.7937 - val_loss: 10.0512 - val_acc: 0.8650\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 6s - loss: 9.8531 - acc: 0.8275 - val_loss: 9.7193 - val_acc: 0.7500\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 6s - loss: 9.5521 - acc: 0.7875 - val_loss: 9.4277 - val_acc: 0.6900\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 6s - loss: 9.1831 - acc: 0.7688 - val_loss: 8.9751 - val_acc: 0.8400\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 5s - loss: 8.7638 - acc: 0.8263 - val_loss: 8.6319 - val_acc: 0.7900\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 6s - loss: 8.5224 - acc: 0.7900 - val_loss: 8.3118 - val_acc: 0.7900\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 6s - loss: 8.1319 - acc: 0.8237 - val_loss: 8.1387 - val_acc: 0.6450\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 7s - loss: 31.0232 - acc: 0.5450 - val_loss: 29.8221 - val_acc: 0.5450\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 6s - loss: 29.0162 - acc: 0.5775 - val_loss: 28.0775 - val_acc: 0.6650\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 5s - loss: 27.3247 - acc: 0.6325 - val_loss: 26.4263 - val_acc: 0.8350\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 6s - loss: 25.7092 - acc: 0.6550 - val_loss: 24.9212 - val_acc: 0.5200\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 5s - loss: 24.2367 - acc: 0.6325 - val_loss: 23.4407 - val_acc: 0.6800\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 5s - loss: 22.7731 - acc: 0.7238 - val_loss: 22.0318 - val_acc: 0.8300\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 6s - loss: 21.4832 - acc: 0.6875 - val_loss: 20.8093 - val_acc: 0.6100\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 5s - loss: 20.2277 - acc: 0.7150 - val_loss: 19.5606 - val_acc: 0.7650\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 5s - loss: 19.0221 - acc: 0.7700 - val_loss: 18.5352 - val_acc: 0.6550\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 5s - loss: 17.8977 - acc: 0.7762 - val_loss: 17.3198 - val_acc: 0.8650\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 6s - loss: 16.8953 - acc: 0.7512 - val_loss: 16.4907 - val_acc: 0.5800\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 6s - loss: 15.9870 - acc: 0.7263 - val_loss: 15.5984 - val_acc: 0.5550\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 6s - loss: 15.0412 - acc: 0.7563 - val_loss: 14.7067 - val_acc: 0.6550\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 6s - loss: 14.1958 - acc: 0.7362 - val_loss: 13.8516 - val_acc: 0.5750\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 6s - loss: 13.3269 - acc: 0.7950 - val_loss: 12.9384 - val_acc: 0.7800\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 5s - loss: 12.6540 - acc: 0.7350 - val_loss: 12.2618 - val_acc: 0.7300\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 6s - loss: 11.8300 - acc: 0.8050 - val_loss: 11.8212 - val_acc: 0.5800\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 6s - loss: 11.1971 - acc: 0.7725 - val_loss: 10.8373 - val_acc: 0.8200\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 5s - loss: 10.5594 - acc: 0.7725 - val_loss: 10.1772 - val_acc: 0.8500\n",
      "Epoch 20/25\n",
      "576/800 [====================>.........] - ETA: 1s - loss: 10.0527 - acc: 0.7934"
     ]
    }
   ],
   "source": [
    "#plotting relu model test accuracy as a function of regularization parameter\n",
    "test_data = np.load(open('bottleneck_features_test.npy','rb'))\n",
    "\n",
    "test_acc=[]\n",
    "reg_values=[]\n",
    "for reg in range(1, 100, 20):\n",
    "    train_relu_modelwithWreg(reg/1000)\n",
    "    top_model = load_relu_modelwithWreg(test_data.shape[1:])\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    top_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    scores = top_model.evaluate(test_data, test_labels, verbose=0)\n",
    "    reg_values.append(reg/1000)\n",
    "    test_acc.append(scores[1]*100)\n",
    "    \n",
    "plt.plot(reg_values,test_acc,'r',label = \"Test accuracy\") \n",
    "plt.xlabel(\"Regularization parameter\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Sensitivity of the model\")\n",
    "#plt.text(100, max(train_acc[2])-max(train_acc[2]/10), 'Test accuracy' ,color = 'green')\n",
    "#plt.text(100, max(train_acc[2])-max(train_acc[2]/8), 'Train accuracy' ,color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
